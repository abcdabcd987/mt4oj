%# -*- coding: utf-8-unix -*-

@article{Kurnia2001,
abstract = {This report describes and evaluates the implementation and applicability of an automatic programming assignment grading system we named the online judge. We compared this with the manual grading system that is currently being used and showed that the automatic grading system, when implemented carefully, is more convenient, fairer, and more secure than the former. We have successfully tested the system on two courses. However, further studies need to be conducted to improve the effectiveness of learning through this system. {\#}},
author = {Kurnia, Andy and Lim, Andrew and Cheang, Brenda},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Online Judge - Kurnia, Lim, Cheang - 2001.pdf:pdf},
journal = {Computers {\&} Education},
keywords = {Automatic grading system,Online judge,Programming assignments},
mendeley-groups = {Undergraduate Thesis},
number = {36},
pages = {299--315},
title = {{Online Judge}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.4626{\&}rep=rep1{\&}type=pdf},
year = {2001}
}


@article{Li2005,
abstract = {POJ ( Peking University Online Judge ) system is aiming at training the ACM/ICPC ( Association of Comuting Machinery/Intenational Collegiate Programming Contest) PKU team members. The system functions include user management, task library, realtime submission and judgement, discussion board, email system. It can he widely used in programming related courses in helping with the homework remarking and online examina-tion. In Peking University, POJ has been integrated in some courses such as 《Introduction to Computing》, 《C + + programming》, 《Data Structure and Algorithms》 and 《Problem solving and Programming》. Through POJ, the students may submit their homework at anytime and get prompt remark to their submisions. Professors can al-so watch the students behaviours on POJ and find out the most common problems with the students in learning. Taking exams on POJ makes the students try their best to improve their online programming techniques in stead of just remembering all the things. Meanwhile it is easier to find out similar programs so that less students copy their homework from others. POJ as a smart tool is a good helper in teaching programming.},
author = {Li, Wen-xin and Guo, Wei},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Peking University Oneline Judge and Its Applications - Li, Guo - 2005.pdf:pdf},
journal = {Journal of Changchun Post and Telecommunication Institute},
mendeley-groups = {Undergraduate Thesis},
title = {{Peking University Oneline Judge and Its Applications}},
url = {http://en.cnki.com.cn/Article{\_}en/CJFDTOTAL-CCYD2005S2046.htm},
volume = {S2},
year = {2005}
}


@article{Simard2017,
abstract = {The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. While machine learning focuses on creating new algorithms and improving the accuracy of "learners", the machine teaching discipline focuses on the efficacy of the "teachers". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.},
archivePrefix = {arXiv},
arxivId = {1707.06742},
author = {Simard, Patrice Y. and Amershi, Saleema and Chickering, David M. and Pelton, Alicia Edelman and Ghorashi, Soroush and Meek, Christopher and Ramos, Gonzalo and Suh, Jina and Verwey, Johan and Wang, Mo and Wernsing, John},
eprint = {1707.06742},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Machine Teaching A New Paradigm for Building Machine Learning Systems - Simard et al. - 2017.pdf:pdf},
mendeley-groups = {Undergraduate Thesis},
title = {{Machine Teaching: A New Paradigm for Building Machine Learning Systems}},
url = {http://arxiv.org/abs/1707.06742},
year = {2017}
}

@article{Zhu2018,
abstract = {In this paper we try to organize machine teaching as a coherent set of ideas. Each idea is presented as varying along a dimension. The collection of dimensions then form the problem space of machine teaching, such that existing teaching problems can be characterized in this space. We hope this organization allows us to gain deeper understanding of individual teaching problems, discover connections among them, and identify gaps in the field.},
archivePrefix = {arXiv},
arxivId = {1801.05927},
author = {Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and Rafferty, Anna N.},
eprint = {1801.05927},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/An Overview of Machine Teaching - Zhu et al. - 2018.pdf:pdf},
mendeley-groups = {Undergraduate Thesis},
title = {{An Overview of Machine Teaching}},
year = {2018}
}

@article{Suh2016,
abstract = {Mixed-initiative classifier training, where the hu-man teacher can choose which items to label or to label items chosen by the computer, has enjoyed empirical success but without a rigor-ous statistical learning theoretical justification. We analyze the label complexity of a simple mixed-initiative training mechanism using teach-ing dimension and active learning. We show that mixed-initiative training is advantageous com-pared to either computer-initiated (represented by active learning) or human-initiated classifier training. The advantage exists across all human teaching abilities, from optimal to completely unhelpful teachers. We further improve classifier training by educating the human teachers. This is done by showing, or explaining, optimal teaching sets to the human teachers. We conduct Mechani-cal Turk human experiments on two stylistic clas-sifier training tasks to illustrate our approach.},
author = {Suh, Jina and Com, Samershi Microsoft},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/The Label Complexity of Mixed-Initiative Classifier Training - Suh, Com - 2016.pdf:pdf},
isbn = {9781510829008},
journal = {Icml},
mendeley-groups = {Undergraduate Thesis},
title = {{The Label Complexity of Mixed-Initiative Classifier Training}},
year = {2016}
}

@article{Alfeld2016,
abstract = {Forecasting models play a key role in money-making ventures in many different markets. Such models are of-ten trained on data from various sources, some of which may be untrustworthy. An actor in a given market may be incentivised to drive predictions in a certain direction to their own benefit. Prior analyses of intelligent adver-saries in a machine-learning context have focused on re-gression and classification. In this paper we address the non-iid setting of time series forecasting. We consider a forecaster, Bob, using a fixed, known model and a re-cursive forecasting method. An adversary, Alice, aims to pull Bob's forecasts toward her desired target series, and may exercise limited influence on the initial val-ues fed into Bob's model. We consider the class of lin-ear autoregressive models, and a flexible framework of encoding Alice's desires and constraints. We describe a method of calculating Alice's optimal attack that is computationally tractable, and empirically demonstrate its effectiveness compared to random and greedy base-lines on synthetic and real-world time series data. We conclude by discussing defensive strategies in the face of Alice-like adversaries.},
author = {Alfeld, Scott and Zhu, Xiaojin and Barford, Paul},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Data Poisoning Attacks against Autoregressive Models - Alfeld, Zhu, Barford - 2016.pdf:pdf},
isbn = {9781577357605},
journal = {Aaai},
keywords = {Technical Papers: Machine Learning Methods},
mendeley-groups = {Undergraduate Thesis},
title = {{Data Poisoning Attacks against Autoregressive Models}},
year = {2016}
}

@article{Mei2015,
abstract = {We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understand-ing training-set attacks is important as more intelli-gent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This pa-per identifies the optimal training-set attack on a broad family of machine learners. First we show that opti-mal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gra-dient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support Vector Machines, logistic regression, and linear regres-sion with extensive experiments. Finally, we discuss po-tential defenses against such attacks.},
author = {Mei, Shike and Zhu, Xiaojin},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners - Mei, Zhu - 2015.pdf:pdf},
isbn = {9781577357025},
journal = {Twenty-Ninth AAAI Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms Track},
mendeley-groups = {Undergraduate Thesis},
pages = {2871--2877},
title = {{Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners}},
url = {http://pages.cs.wisc.edu/{~}jerryzhu/pub/Mei2015Machine.pdf},
year = {2015}
}

@article{Whitehill2017,
author = {Whitehill, Jacob and Movellan, Javier},
doi = {10.1109/TLT.2017.2692761},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Approximately Optimal Teaching of Approximately Optimal Learners - Whitehill, Movellan - 2017.pdf:pdf},
issn = {1939-1382},
journal = {IEEE Transactions on Learning Technologies},
mendeley-groups = {Undergraduate Thesis},
number = {9},
pages = {1--1},
title = {{Approximately Optimal Teaching of Approximately Optimal Learners}},
url = {http://ieeexplore.ieee.org/document/7895197/},
volume = {13},
year = {2017}
}

@article{Patil2014,
abstract = {Basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli. Recent work finds that people's category judgments are guided by a small set of examples that are retrieved from memory at decision time. This limited and stochastic retrieval places limits on human performance for proba-bilistic classification decisions. In light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items. One shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion. In this contribution, we take a first principles approach to constructing idealized training sets. We apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are). As predicted, we find that the machine teacher recommends idealized training sets. We also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model. As predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective. Our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.},
author = {Patil, Kaustubh Raosaheb and Love, Bradley C},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Optimal Teaching for Limited-Capacity Human Learners - Patil, Love - 2014.pdf:pdf},
issn = {10495258},
journal = {Nips},
mendeley-groups = {Undergraduate Thesis},
pages = {1--9},
title = {{Optimal Teaching for Limited-Capacity Human Learners}},
year = {2014}
}

@book{Mitchell1997,
author    = {Tom Mitchell},
title     = {Machine Learning},
publisher = {McGraw Hill},
year      = 1997,
isbn      = {0070428077}
}

@article{Ng2000,
abstract = {Before there were workshops and degrees, how did aspiring writers learn to write? By reading the work of their predecessors and contemporaries, says author and teacher Prose. Prose invites you on a guided tour of the tools and the tricks of the masters. She reads the very best writers and discovers why their work has endured. She takes pleasure in the magnificent sentences of Philip Roth and the breathtaking paragraphs of Isaac Babel; she is moved by the brilliant characterization in George Eliot's Middlemarch. She looks to John Le Carre for how to advance plot through dialogue, to Flannery O'Connor for the cunning use of the telling detail, and to James Joyce and Katherine Mansfield for clever examples of how to employ gesture to create character. She cautions readers to slow down and pay attention to words, the raw material out of which literature is crafted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ng, Andrew},
doi = {10.1111/j.1466-8238.2009.00506.x},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0226206807},
issn = {02660830},
journal = {CS229 Lecture notes},
mendeley-groups = {Undergraduate Thesis},
number = {1},
pages = {1--3},
pmid = {21889629},
title = {{CS229 Lecture notes}},
url = {http://cs229.stanford.edu/notes/cs229-notes1.pdf http://www.stanford.edu/class/cs229/},
volume = {1},
year = {2000}
}

@book{Ricci2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
booktitle = {CEUR Workshop Proceedings},
doi = {10.1007/978-0-387-85820-3},
editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-387-85819-7},
issn = {16130073},
mendeley-groups = {Undergraduate Thesis},
pmid = {21607264},
publisher = {Springer US},
title = {{Recommender Systems Handbook}},
url = {http://link.springer.com/10.1007/978-0-387-85820-3},
volume = {532},
year = {2011}
}

@article{Rai2011,
author = {Rai, Piyush},
file = {::},
keywords = {()},
mendeley-groups = {Undergraduate Thesis},
title = {{Kernel Methods and Nonlinear Classification}},
url = {https://www.cs.utah.edu/{~}piyush/teaching/15-9-print.pdf},
year = {2011}
}

@inproceedings{Rendle2010,
abstract = {In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models.},
author = {Rendle, Steffen},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2010.127},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Factorization machines - Rendle - 2010.pdf:pdf},
isbn = {9780769542560},
issn = {15504786},
keywords = {Factorization machine,Sparse data,Support vector machine,Tensor factorization},
mendeley-groups = {Undergraduate Thesis},
pages = {995--1000},
pmid = {1530748},
title = {{Factorization machines}},
url = {https://www.csie.ntu.edu.tw/{~}b97053/paper/Rendle2010FM.pdf},
year = {2010}
}

@article{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for ruining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Friedman, Jerome H},
doi = {DOI 10.1214/aos/1013203451},
eprint = {arXiv:1011.1669v3},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Greedy function approximation A gradient boosting machine - Friedman - 2001.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Boosting,Decision trees,Function estimation,Robust nonparametric regression},
mendeley-groups = {Undergraduate Thesis},
number = {5},
pages = {1189--1232},
pmid = {21740230},
title = {{Greedy function approximation: A gradient boosting machine}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aos/1013203451},
volume = {29},
year = {2001}
}

@article{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan-tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres-sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/XGBoost - Chen, Guestrin - 2016.pdf:pdf},
isbn = {9781450342322},
journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
keywords = {large,scale machine learning},
mendeley-groups = {Undergraduate Thesis},
pages = {785--794},
title = {{XGBoost}},
url = {http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf?ip=103.193.128.209{\&}id=2939785{\&}acc=CHORUS{\&}key=4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35.6D218144511F3437{\&}{\_}{\_}acm{\_}{\_}=1526811465{\_}6b20c3893aebaf5d394453204b3f17ed http://dl.acm.org/citation.cfm?doid=2939672.2939785},
year = {2016}
}

@inproceedings{Pascanu2013,
abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1211.5063},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
booktitle = {International Conference on Machine Learning},
doi = {10.1109/72.279181},
eprint = {1211.5063},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/On the difficulty of training Recurrent Neural Networks - Pascanu, Mikolov, Bengio - 2013.pdf:pdf},
issn = {1045-9227},
mendeley-groups = {Undergraduate Thesis},
pmid = {18267787},
title = {{On the difficulty of training Recurrent Neural Networks}},
url = {http://proceedings.mlr.press/v28/pascanu13.pdf http://arxiv.org/abs/1211.5063},
year = {2013}
}

@misc{Ola2015,
abstract = {This page describes the idea behind LSTM},
author = {Ola, Christopher},
booktitle = {2015-08-27},
mendeley-groups = {Undergraduate Thesis},
pages = {1},
title = {{Understanding LSTM Networks -- colah's blog}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2018-05-21},
year = {2015}
}


@article{mnih_asynchronous_2016,
    title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
    url = {http://arxiv.org/abs/1602.01783},
    abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
    language = {en},
    urldate = {2018-05-21},
    journal = {arXiv:1602.01783 [cs]},
    author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
    month = feb,
    year = {2016},
    note = {arXiv: 1602.01783},
    keywords = {Computer Science - Learning},
    file = {Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:/Users/abcdabcd987/Documents/Zotero/storage/AEAFWALG/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf}
}

@article{tsitsiklis_analysis_1997,
    title = {An analysis of temporal-difference learning with function approximation},
    volume = {42},
    issn = {00189286},
    url = {http://ieeexplore.ieee.org/document/580874/},
    doi = {10.1109/9.580874},
    abstract = {We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an inﬁnite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a ﬁnite or inﬁnite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning.},
    language = {en},
    number = {5},
    urldate = {2018-05-21},
    journal = {IEEE Transactions on Automatic Control},
    author = {Tsitsiklis, J.N. and Van Roy, B.},
    month = may,
    year = {1997},
    pages = {674--690},
    file = {Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with f.pdf:/Users/abcdabcd987/Documents/Zotero/storage/6FT2BJS7/Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with f.pdf:application/pdf}
}

@article{mnih_human-level_2015,
    title = {Human-level control through deep reinforcement learning},
    volume = {518},
    issn = {0028-0836, 1476-4687},
    url = {http://www.nature.com/articles/nature14236},
    doi = {10.1038/nature14236},
    language = {en},
    number = {7540},
    urldate = {2018-05-21},
    journal = {Nature},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
    month = feb,
    year = {2015},
    pages = {529--533},
    file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/Users/abcdabcd987/Documents/Zotero/storage/B5WWFLB7/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}


