%# -*- coding: utf-8-unix -*-

@article{Kurnia2001,
abstract = {This report describes and evaluates the implementation and applicability of an automatic programming assignment grading system we named the online judge. We compared this with the manual grading system that is currently being used and showed that the automatic grading system, when implemented carefully, is more convenient, fairer, and more secure than the former. We have successfully tested the system on two courses. However, further studies need to be conducted to improve the effectiveness of learning through this system. {\#}},
author = {Kurnia, Andy and Lim, Andrew and Cheang, Brenda},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Online Judge - Kurnia, Lim, Cheang - 2001.pdf:pdf},
journal = {Computers {\&} Education},
keywords = {Automatic grading system,Online judge,Programming assignments},
mendeley-groups = {Undergraduate Thesis},
number = {36},
pages = {299--315},
title = {{Online Judge}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.4626{\&}rep=rep1{\&}type=pdf},
year = {2001}
}


@article{Li2005,
abstract = {POJ ( Peking University Online Judge ) system is aiming at training the ACM/ICPC ( Association of Comuting Machinery/Intenational Collegiate Programming Contest) PKU team members. The system functions include user management, task library, realtime submission and judgement, discussion board, email system. It can he widely used in programming related courses in helping with the homework remarking and online examina-tion. In Peking University, POJ has been integrated in some courses such as 《Introduction to Computing》, 《C + + programming》, 《Data Structure and Algorithms》 and 《Problem solving and Programming》. Through POJ, the students may submit their homework at anytime and get prompt remark to their submisions. Professors can al-so watch the students behaviours on POJ and find out the most common problems with the students in learning. Taking exams on POJ makes the students try their best to improve their online programming techniques in stead of just remembering all the things. Meanwhile it is easier to find out similar programs so that less students copy their homework from others. POJ as a smart tool is a good helper in teaching programming.},
author = {Li, Wen-xin and Guo, Wei},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Peking University Oneline Judge and Its Applications - Li, Guo - 2005.pdf:pdf},
journal = {Journal of Changchun Post and Telecommunication Institute},
mendeley-groups = {Undergraduate Thesis},
title = {{Peking University Oneline Judge and Its Applications}},
url = {http://en.cnki.com.cn/Article{\_}en/CJFDTOTAL-CCYD2005S2046.htm},
volume = {S2},
year = {2005}
}

@inproceedings{luo2008programming,
  title={Programming grid: a computer-aided education system for programming courses based on online judge},
  author={Luo, Yingwei and Wang, Xiaolin and Zhang, Zhengyi},
  booktitle={Proceedings of the 1st ACM Summit on Computing Education in China on First ACM Summit on Computing Education in China},
  pages={10},
  year={2008},
  organization={ACM}
}

@inproceedings{kosowski2007application,
  title={Application of an online judge \& contester system in academic tuition},
  author={Kosowski, Adrian and Ma{\l}afiejski, Micha{\l} and Noi{\'n}ski, Tomasz},
  booktitle={International Conference on Web-Based Learning},
  pages={343--354},
  year={2007},
  organization={Springer}
}

@inproceedings{bez2014uri,
  title={URI Online Judge Academic: A tool for algorithms and programming classes},
  author={Bez, Jean Luca and Tonin, Neilor A and Rodegheri, Paulo R},
  booktitle={Computer Science \& Education (ICCSE), 2014 9th International Conference on},
  pages={149--152},
  year={2014},
  organization={IEEE}
}

@article{Simard2017,
abstract = {The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. While machine learning focuses on creating new algorithms and improving the accuracy of "learners", the machine teaching discipline focuses on the efficacy of the "teachers". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.},
archivePrefix = {arXiv},
arxivId = {1707.06742},
author = {Simard, Patrice Y. and Amershi, Saleema and Chickering, David M. and Pelton, Alicia Edelman and Ghorashi, Soroush and Meek, Christopher and Ramos, Gonzalo and Suh, Jina and Verwey, Johan and Wang, Mo and Wernsing, John},
eprint = {1707.06742},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Machine Teaching A New Paradigm for Building Machine Learning Systems - Simard et al. - 2017.pdf:pdf},
mendeley-groups = {Undergraduate Thesis},
title = {{Machine Teaching: A New Paradigm for Building Machine Learning Systems}},
url = {http://arxiv.org/abs/1707.06742},
year = {2017}
}

@article{Zhu2018,
abstract = {In this paper we try to organize machine teaching as a coherent set of ideas. Each idea is presented as varying along a dimension. The collection of dimensions then form the problem space of machine teaching, such that existing teaching problems can be characterized in this space. We hope this organization allows us to gain deeper understanding of individual teaching problems, discover connections among them, and identify gaps in the field.},
archivePrefix = {arXiv},
arxivId = {1801.05927},
author = {Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and Rafferty, Anna N.},
eprint = {1801.05927},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/An Overview of Machine Teaching - Zhu et al. - 2018.pdf:pdf},
mendeley-groups = {Undergraduate Thesis},
title = {{An Overview of Machine Teaching}},
year = {2018}
}

@article{Suh2016,
abstract = {Mixed-initiative classifier training, where the hu-man teacher can choose which items to label or to label items chosen by the computer, has enjoyed empirical success but without a rigor-ous statistical learning theoretical justification. We analyze the label complexity of a simple mixed-initiative training mechanism using teach-ing dimension and active learning. We show that mixed-initiative training is advantageous com-pared to either computer-initiated (represented by active learning) or human-initiated classifier training. The advantage exists across all human teaching abilities, from optimal to completely unhelpful teachers. We further improve classifier training by educating the human teachers. This is done by showing, or explaining, optimal teaching sets to the human teachers. We conduct Mechani-cal Turk human experiments on two stylistic clas-sifier training tasks to illustrate our approach.},
author = {Suh, Jina and Com, Samershi Microsoft},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/The Label Complexity of Mixed-Initiative Classifier Training - Suh, Com - 2016.pdf:pdf},
isbn = {9781510829008},
journal = {Icml},
mendeley-groups = {Undergraduate Thesis},
title = {{The Label Complexity of Mixed-Initiative Classifier Training}},
year = {2016}
}

@article{Alfeld2016,
abstract = {Forecasting models play a key role in money-making ventures in many different markets. Such models are of-ten trained on data from various sources, some of which may be untrustworthy. An actor in a given market may be incentivised to drive predictions in a certain direction to their own benefit. Prior analyses of intelligent adver-saries in a machine-learning context have focused on re-gression and classification. In this paper we address the non-iid setting of time series forecasting. We consider a forecaster, Bob, using a fixed, known model and a re-cursive forecasting method. An adversary, Alice, aims to pull Bob's forecasts toward her desired target series, and may exercise limited influence on the initial val-ues fed into Bob's model. We consider the class of lin-ear autoregressive models, and a flexible framework of encoding Alice's desires and constraints. We describe a method of calculating Alice's optimal attack that is computationally tractable, and empirically demonstrate its effectiveness compared to random and greedy base-lines on synthetic and real-world time series data. We conclude by discussing defensive strategies in the face of Alice-like adversaries.},
author = {Alfeld, Scott and Zhu, Xiaojin and Barford, Paul},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Data Poisoning Attacks against Autoregressive Models - Alfeld, Zhu, Barford - 2016.pdf:pdf},
isbn = {9781577357605},
journal = {Aaai},
keywords = {Technical Papers: Machine Learning Methods},
mendeley-groups = {Undergraduate Thesis},
title = {{Data Poisoning Attacks against Autoregressive Models}},
year = {2016}
}

@article{Mei2015,
abstract = {We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understand-ing training-set attacks is important as more intelli-gent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This pa-per identifies the optimal training-set attack on a broad family of machine learners. First we show that opti-mal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gra-dient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support Vector Machines, logistic regression, and linear regres-sion with extensive experiments. Finally, we discuss po-tential defenses against such attacks.},
author = {Mei, Shike and Zhu, Xiaojin},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners - Mei, Zhu - 2015.pdf:pdf},
isbn = {9781577357025},
journal = {Twenty-Ninth AAAI Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms Track},
mendeley-groups = {Undergraduate Thesis},
pages = {2871--2877},
title = {{Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners}},
url = {http://pages.cs.wisc.edu/{~}jerryzhu/pub/Mei2015Machine.pdf},
year = {2015}
}

@article{Whitehill2017,
author = {Whitehill, Jacob and Movellan, Javier},
doi = {10.1109/TLT.2017.2692761},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Approximately Optimal Teaching of Approximately Optimal Learners - Whitehill, Movellan - 2017.pdf:pdf},
issn = {1939-1382},
journal = {IEEE Transactions on Learning Technologies},
mendeley-groups = {Undergraduate Thesis},
number = {9},
pages = {1--1},
title = {{Approximately Optimal Teaching of Approximately Optimal Learners}},
url = {http://ieeexplore.ieee.org/document/7895197/},
volume = {13},
year = {2017}
}

@inproceedings{Patil2014,
  title={Optimal teaching for limited-capacity human learners},
  author={Patil, Kaustubh R and Zhu, Xiaojin and Kope{\'c}, {\L}ukasz and Love, Bradley C},
  booktitle={Advances in neural information processing systems},
  pages={2465--2473},
  year={2014}
}

@book{Mitchell1997,
author    = {Tom Mitchell},
title     = {Machine Learning},
publisher = {McGraw Hill},
year      = 1997,
isbn      = {0070428077}
}

@article{Ng2000,
abstract = {Before there were workshops and degrees, how did aspiring writers learn to write? By reading the work of their predecessors and contemporaries, says author and teacher Prose. Prose invites you on a guided tour of the tools and the tricks of the masters. She reads the very best writers and discovers why their work has endured. She takes pleasure in the magnificent sentences of Philip Roth and the breathtaking paragraphs of Isaac Babel; she is moved by the brilliant characterization in George Eliot's Middlemarch. She looks to John Le Carre for how to advance plot through dialogue, to Flannery O'Connor for the cunning use of the telling detail, and to James Joyce and Katherine Mansfield for clever examples of how to employ gesture to create character. She cautions readers to slow down and pay attention to words, the raw material out of which literature is crafted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ng, Andrew},
doi = {10.1111/j.1466-8238.2009.00506.x},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0226206807},
issn = {02660830},
journal = {CS229 Lecture notes},
mendeley-groups = {Undergraduate Thesis},
number = {1},
pages = {1--3},
pmid = {21889629},
title = {{CS229 Lecture notes}},
url = {http://cs229.stanford.edu/notes/cs229-notes1.pdf http://www.stanford.edu/class/cs229/},
volume = {1},
year = {2000}
}

@book{Ricci2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
booktitle = {CEUR Workshop Proceedings},
doi = {10.1007/978-0-387-85820-3},
editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-387-85819-7},
issn = {16130073},
mendeley-groups = {Undergraduate Thesis},
pmid = {21607264},
publisher = {Springer US},
title = {{Recommender Systems Handbook}},
url = {http://link.springer.com/10.1007/978-0-387-85820-3},
volume = {532},
year = {2011}
}

@article{Rai2011,
author = {Rai, Piyush},
file = {::},
keywords = {()},
mendeley-groups = {Undergraduate Thesis},
title = {{Kernel Methods and Nonlinear Classification}},
url = {https://www.cs.utah.edu/{~}piyush/teaching/15-9-print.pdf},
year = {2011}
}

@inproceedings{Rendle2010,
abstract = {In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models.},
author = {Rendle, Steffen},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2010.127},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Factorization machines - Rendle - 2010.pdf:pdf},
isbn = {9780769542560},
issn = {15504786},
keywords = {Factorization machine,Sparse data,Support vector machine,Tensor factorization},
mendeley-groups = {Undergraduate Thesis},
pages = {995--1000},
pmid = {1530748},
title = {{Factorization machines}},
url = {https://www.csie.ntu.edu.tw/{~}b97053/paper/Rendle2010FM.pdf},
year = {2010}
}

@article{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for ruining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Friedman, Jerome H},
doi = {DOI 10.1214/aos/1013203451},
eprint = {arXiv:1011.1669v3},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Greedy function approximation A gradient boosting machine - Friedman - 2001.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Boosting,Decision trees,Function estimation,Robust nonparametric regression},
mendeley-groups = {Undergraduate Thesis},
number = {5},
pages = {1189--1232},
pmid = {21740230},
title = {{Greedy function approximation: A gradient boosting machine}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aos/1013203451},
volume = {29},
year = {2001}
}

@article{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan-tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres-sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/XGBoost - Chen, Guestrin - 2016.pdf:pdf},
isbn = {9781450342322},
journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
keywords = {large,scale machine learning},
mendeley-groups = {Undergraduate Thesis},
pages = {785--794},
title = {{XGBoost}},
url = {http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf?ip=103.193.128.209{\&}id=2939785{\&}acc=CHORUS{\&}key=4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35.6D218144511F3437{\&}{\_}{\_}acm{\_}{\_}=1526811465{\_}6b20c3893aebaf5d394453204b3f17ed http://dl.acm.org/citation.cfm?doid=2939672.2939785},
year = {2016}
}

@inproceedings{Pascanu2013,
abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1211.5063},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
booktitle = {International Conference on Machine Learning},
doi = {10.1109/72.279181},
eprint = {1211.5063},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/On the difficulty of training Recurrent Neural Networks - Pascanu, Mikolov, Bengio - 2013.pdf:pdf},
issn = {1045-9227},
mendeley-groups = {Undergraduate Thesis},
pmid = {18267787},
title = {{On the difficulty of training Recurrent Neural Networks}},
url = {http://proceedings.mlr.press/v28/pascanu13.pdf http://arxiv.org/abs/1211.5063},
year = {2013}
}

@misc{Ola2015,
abstract = {This page describes the idea behind LSTM},
author = {Ola, Christopher},
booktitle = {2015-08-27},
mendeley-groups = {Undergraduate Thesis},
pages = {1},
title = {{Understanding LSTM Networks -- colah's blog}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2018-05-21},
year = {2015}
}


@article{mnih_asynchronous_2016,
    title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
    url = {http://arxiv.org/abs/1602.01783},
    abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
    language = {en},
    urldate = {2018-05-21},
    journal = {arXiv:1602.01783 [cs]},
    author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
    month = feb,
    year = {2016},
    note = {arXiv: 1602.01783},
    keywords = {Computer Science - Learning},
    file = {Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:/Users/abcdabcd987/Documents/Zotero/storage/AEAFWALG/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf}
}

@article{tsitsiklis_analysis_1997,
    title = {An analysis of temporal-difference learning with function approximation},
    volume = {42},
    issn = {00189286},
    url = {http://ieeexplore.ieee.org/document/580874/},
    doi = {10.1109/9.580874},
    abstract = {We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an inﬁnite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a ﬁnite or inﬁnite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning.},
    language = {en},
    number = {5},
    urldate = {2018-05-21},
    journal = {IEEE Transactions on Automatic Control},
    author = {Tsitsiklis, J.N. and Van Roy, B.},
    month = may,
    year = {1997},
    pages = {674--690},
    file = {Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with f.pdf:/Users/abcdabcd987/Documents/Zotero/storage/6FT2BJS7/Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with f.pdf:application/pdf}
}

@article{mnih_human-level_2015,
    title = {Human-level control through deep reinforcement learning},
    volume = {518},
    issn = {0028-0836, 1476-4687},
    url = {http://www.nature.com/articles/nature14236},
    doi = {10.1038/nature14236},
    language = {en},
    number = {7540},
    urldate = {2018-05-21},
    journal = {Nature},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
    month = feb,
    year = {2015},
    pages = {529--533},
    file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/Users/abcdabcd987/Documents/Zotero/storage/B5WWFLB7/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}

@article{bayer_fastfm:_2016,
    title = {{fastFM}: {A} {Library} for {Factorization} {Machines}},
    volume = {17},
    url = {http://jmlr.org/papers/v17/15-355.html},
    number = {184},
    journal = {Journal of Machine Learning Research},
    author = {Bayer, Immanuel},
    year = {2016},
    pages = {1--5}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@article{kingma_adam:_2014,
    title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
    shorttitle = {Adam},
    url = {http://arxiv.org/abs/1412.6980},
    abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
    urldate = {2018-05-22},
    journal = {arXiv:1412.6980 [cs]},
    author = {Kingma, Diederik P. and Ba, Jimmy},
    month = dec,
    year = {2014},
    note = {arXiv: 1412.6980},
    keywords = {Computer Science - Learning},
    annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
    file = {arXiv\:1412.6980 PDF:/Users/abcdabcd987/Documents/Zotero/storage/9BH3XM9B/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/abcdabcd987/Documents/Zotero/storage/M8G9YQJQ/1412.html:text/html}
}

@article{brockman_openai_2016,
    title = {{OpenAI} {Gym}},
    url = {http://arxiv.org/abs/1606.01540},
    abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
    urldate = {2018-05-22},
    journal = {arXiv:1606.01540 [cs]},
    author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
    month = jun,
    year = {2016},
    note = {arXiv: 1606.01540},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning},
    file = {arXiv\:1606.01540 PDF:/Users/abcdabcd987/Documents/Zotero/storage/6IKG2B3Y/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/Users/abcdabcd987/Documents/Zotero/storage/CIC432WF/1606.html:text/html}
}

@article{ioffe_batch_2015,
    title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
    shorttitle = {Batch {Normalization}},
    url = {http://arxiv.org/abs/1502.03167},
    abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
    urldate = {2018-05-23},
    journal = {arXiv:1502.03167 [cs]},
    author = {Ioffe, Sergey and Szegedy, Christian},
    month = feb,
    year = {2015},
    note = {arXiv: 1502.03167},
    keywords = {Computer Science - Learning},
    file = {arXiv\:1502.03167 PDF:/Users/abcdabcd987/Documents/Zotero/storage/8XQ5KLZ6/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/abcdabcd987/Documents/Zotero/storage/QW9WRPKL/1502.html:text/html}
}

@misc{flach_putting_2007,
    title = {Putting {Things} in {Order}: {On} the {Fundamental} {Role} of {Ranking} in {Classification} and {Probability} {Estimation}},
    shorttitle = {Putting {Things} in {Order}},
    url = {http://videolectures.net/ecml07_flach_pto/},
    language = {en},
    urldate = {2018-05-23},
    author = {Flach, Peter A.},
    month = sep,
    year = {2007},
    file = {Snapshot:/Users/abcdabcd987/Documents/Zotero/storage/5YTLI7V8/ecml07_flach_pto.html:text/html}
}

@article{Zhu2015,
abstract = {I draw the reader's attention to machine teaching, the prob-lem of finding an optimal training set given a machine learn-ing algorithm and a target model. In addition to generating fascinating mathematical questions for computer scientists to ponder, machine teaching holds the promise of enhancing ed-ucation and personnel training. The Socratic dialogue style aims to stimulate critical thinking.},
author = {Zhu, Xiaojin},
file = {:Users/abcdabcd987/Documents/Mendeley Desktop/Machine Teaching An Inverse Problem to Machine Learning and an Approach Toward Optimal Education - Zhu - 2015.pdf:pdf},
journal = {AAAI},
mendeley-groups = {Undergraduate Thesis},
title = {{Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education}},
year = {2015}
}

@inproceedings{zhu2013machine,
  title={Machine teaching for bayesian learners in the exponential family},
  author={Zhu, Xiaojin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1905--1913},
  year={2013}
}

@article{xuezhou_zhang_optimal_2016,
    title = {Optimal {Teaching} for {Online} {Perceptrons}},
    abstract = {Consider a teacher designing a good lecture for students, or a hacker drafting a poisonous text input against Tay the chatterbot. Both cases can be formulated as a task of constructing special training data, such that a known learning algorithm taking the constructed data will arrive at a prespeciﬁed target model. This task is known as optimal teaching, which has a wide range of applications in education, psychology, computer security, program synthesis, etc.. Prior analysis of optimal teaching focused exclusively on batch learners. However, a theoretical understand of optimal teaching for online (sequential) learners is also important for many applications. This paper presents the ﬁrst study of optimal teaching for an online learner, speciﬁcally the perceptron. We show how to construct the shortest input sequence for a perceptron, and prove that the sequence has length one when the teacher knows everything about the perceptron, or length three when the teacher does not know the initial weight vector of the perceptron.},
    language = {en},
    author = {{Xuezhou Zhang} and {Hrag Gorune Ohannessian} and {Ayon Sen} and {Scott Alfeld} and {Xiaojin Zhu}},
    year = {2016},
    pages = {6},
    file = {Xuezhou Zhang et al. - Optimal Teaching for Online Perceptrons.pdf:/Users/abcdabcd987/Documents/Zotero/storage/MB83JPL4/Xuezhou Zhang et al. - Optimal Teaching for Online Perceptrons.pdf:application/pdf}
}

@article{liu2016teaching,
  title={The teaching dimension of linear learners},
  author={Liu, Ji and Zhu, Xiaojin},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={162},
  pages={1--25},
  year={2016}
}

@inproceedings{zhu2017no,
  title={No Learner Left Behind: On the Complexity of Teaching Multiple Learners Simultaneously},
  author={Zhu, Xiaojin and Liu, Ji and Lopes, Manuel},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={3588--3594},
  year={2017},
  organization={AAAI Press}
}

@article{ma2018teacher,
  title={Teacher Improves Learning by Selecting a Training Subset},
  author={Ma, Yuzhe and Nowak, Robert and Rigollet, Philippe and Zhang, Xuezhou and Zhu, Xiaojin},
  journal={arXiv preprint arXiv:1802.08946},
  year={2018}
}

@inproceedings{khan2011humans,
  title={How do humans teach: On curriculum learning and teaching dimension},
  author={Khan, Faisal and Mutlu, Bilge and Zhu, Xiaojin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1449--1457},
  year={2011}
}

@article{ebbinghaus2013memory,
  title={Memory: A contribution to experimental psychology},
  author={Ebbinghaus, Hermann},
  journal={Annals of neurosciences},
  volume={20},
  number={4},
  pages={155},
  year={2013},
  publisher={Karger Publishers}
}

@article{bliss1993synaptic,
  title={A synaptic model of memory: long-term potentiation in the hippocampus},
  author={Bliss, Tim VP and Collingridge, Graham L},
  journal={Nature},
  volume={361},
  number={6407},
  pages={31},
  year={1993},
  publisher={Nature Publishing Group}
}

@inproceedings{reddy2016unbounded,
  title={Unbounded human learning: Optimal scheduling for spaced repetition},
  author={Reddy, Siddharth and Labutov, Igor and Banerjee, Siddhartha and Joachims, Thorsten},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1815--1824},
  year={2016},
  organization={ACM}
}

@article{pimsleur1967memory,
  title={A memory schedule},
  author={Pimsleur, Paul},
  journal={The Modern Language Journal},
  volume={51},
  number={2},
  pages={73--75},
  year={1967},
  publisher={JSTOR}
}

@article{leitner1972so,
  title={So lernt man Lernen: Der Weg zum Erfolg [Learning to learn: The road to success]},
  author={Leitner, S},
  journal={Freiburg: Herder},
  year={1972}
}

@inproceedings{settles2016trainable,
  title={A trainable spaced repetition model for language learning},
  author={Settles, Burr and Meeder, Brendan},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  volume={1},
  pages={1848--1858},
  year={2016}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{rafferty2016faster,
  title={Faster teaching via {POMDP} planning},
  author={Rafferty, Anna N and Brunskill, Emma and Griffiths, Thomas L and Shafto, Patrick},
  journal={Cognitive science},
  volume={40},
  number={6},
  pages={1290--1332},
  year={2016},
  publisher={Wiley Online Library}
}

@article{reddy_accelerating_2017,
    title = {Accelerating {Human} {Learning} with {Deep} {Reinforcement} {Learning}},
    abstract = {Guiding a student through a sequence of lessons and helping them retain knowledge is one of the central challenges in education. Online learning platforms like Khan Academy and Duolingo tackle this problem in part by using interaction data to estimate student proﬁciency and recommend content. While the literature proposes a variety of algorithms for modeling student learning, there is relatively little work on principled methods for sequentially choosing items for the student to review in order to maximize learning. We study this decision problem as an instance of reinforcement learning, and draw on recent advances in training deep neural networks to learn ﬂexible and scalable teaching policies that select the next item to review. Our primary contribution is an analysis of a model-free review scheduling algorithm for spaced repetition systems that does not explicitly model the student, and instead learns a policy that directly operates on raw observations of the study history. As a preliminary study, we train and evaluate this method using a student simulator based on cognitive models of human memory. Results show that modelfree scheduling is competitive against widely-used heuristics like SuperMemo and the Leitner system on various learning objectives and student models.},
    language = {en},
    author = {Reddy, Siddharth and Levine, Sergey and Dragan, Anca},
    year = {2017},
    pages = {9},
    file = {Reddy et al. - Accelerating Human Learning with Deep Reinforcemen.pdf:/Users/abcdabcd987/Documents/Zotero/storage/2HK92HQL/Reddy et al. - Accelerating Human Learning with Deep Reinforcemen.pdf:application/pdf}
}

@misc{tim_bajrin_why_2014,
    title = {Why {Basic} {Coding} {Should} {Be} a {Mandatory} {Class} in {Junior} {High}},
    url = {http://time.com/2881453/programming-in-schools/},
    abstract = {It goes without saying, but understanding how technology works makes it much easier for a person to get the most out of it.},
    language = {en},
    urldate = {2018-05-24},
    journal = {Time},
    author = {{Tim Bajrin}},
    month = jun,
    year = {2014},
    file = {Snapshot:/Users/abcdabcd987/Documents/Zotero/storage/8YXHK7AB/programming-in-schools.html:text/html}
}
